#+title: Bayesian Statistics, course outline
#+author: dr. N.D.  van Foreest
#+date: 2020:09:01

#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage{a4wide}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \setminted[python]{linenos=true}
#+LATEX_HEADER: \setminted[python]{frame=lines}
#+LATEX_HEADER: \usepackage{mathpazo}


#+STARTUP: indent hidestars showall


* Schedule


| Week | Schedule | Topic              | Lecturer |
|------+----------+--------------------+----------|
|    1 |        1 | Econometrics vs ML | avv      |
|      |        2 |                    | avv      |
|    2 |        3 |                    | avv      |
|      |        4 | Kernel methods     | nvf      |
|    3 |        5 |                    | avv      |
|      |        6 |                    | nvf      |
|    4 |        7 |                    | avv      |
|      |        8 |                    | nvf      |
|    5 |        9 |                    | avv      |
|      |       10 |                    | nvf      |

- Since this is the first time we use this book, we need to build up some experience about a reasonable reading load per week.
- The lectures of A van Vuuren (avv)  focus on  theoretical aspects of machine learning
- The lectures of N van Foreest (nvf) focus on computational aspects  of machine learning (implementation in python)


* Course goals

During the course, the student
- Obtains an overview of the main methods used for statistical learning
- Can apply several statistical concepts to assess learning models and selection/classification methods
- Can implement the discussed methods and concepts in working algorithms and code
- Can find and use toolboxes available on-line, e.g. Tensorflow
- Can apply the concepts and methods to real-life problems

* Course description

As this is the last course in the master, we expect students to be able to study a book more or less independently (or otherwise develop this skill very rapidly).
Students are expected to read most of the reading material themselves, and formulate questions on parts they do not understand, of for which they need further context.
During the lectures we will discuss these problems and questions.
Students have to turn in these questions a few days before the actual lecture.

There will be lectures during the first five course weeks.
For the last two weeks of the course, we make groups of two students.
Each group has to choose a technique or method (from one of the books below)  which the members of the group are interested in learning.
The group has to find a problem on the internet with data to which to apply their method of choice.
The group writes a brief report, in LaTeX.
The core of the report is at most 5 pages, with name "groupno-title.pdf".
The contents are like this:

- Cover page: title, student names and student id, course code, year
- Core:
  - Intro, problem description
  - Data description (origin of data, data analysis/outliers, filtering methods applied), discussion of why that data is suitable. Use graphs where possible. Include small tables to explain the format of the data.
  - Methods: Why the method is chosen, suitability for the problem at hand, but also potential shortcomings.
  - Analysis, findings; use and discuss graphs.
  - Conclusion: main findings, quality/reliability of findings, evaluation of chosen methods) and extensions
- Appendix with code whose core parts are documented



* Exam and Grading

There will be a 2 h exam at the end of the course.
This exam will be closed book, and focuses on the theory of the book.
We will ask you to explain
- derivations in the mathematics
- crucial parts/aspects of an algorithm
- strengths and weaknesses of certain methods and tests

If technically possible at the exam, we will also ask you to apply/adapt methods we discussed in class to concrete data.


Let the grades of the exam and report be  $e$ and $r$, respectively, then the final grade will be computed like this.

#+BEGIN_SRC python
def compute_grade(e, r):
    if e < 5 or r < 5:
        g = min(e, r)
    else:
        g = max(0.5 * e + 0.5 * r, e)

    return int(g + 0.5) # rounding
#+END_SRC


* Literature

As background material our primary source is the book of Hastie, Tibshirani and Friedman.
However, you should definitely consult the other sources below; perhaps one of those fits better with your way of understanding things or your interest. For your report, you can use material of any of these books, but include references.

- [[https://web.stanford.edu/~hastie/ElemStatLearn/][Hastie, The Elements of Statistical Learning]]
- [[https://web.stanford.edu/~hastie/CASI/][Hastie and Efron, Computer Age Statistical Inference: Algorithms, Evidence and Data Science]]
- [[https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf][Bishop, Pattern Recognition and Machine Learning]]
- [[https://people.smp.uq.edu.au/DirkKroese/DSML/][Kroeze, Data Science and Machine Learning]]




*  Interesting research topics


- https://www.sciencealert.com/mammal-extinctions-are-speeding-up-with-unprecedented-magnitude-scientists-warn
- https://www.pnas.org/content/114/37/9859
- https://www.theseattledataguy.com/healthcare-fraud-detection-with-python/
- [[https://towardsdatascience.com/learn-python-data-analytics-by-example-ny-parking-violations-e1ce1847fa2]]
- [[https://machinelearningmastery.com/random-forest-for-time-series-forecasting/]]
- [[https://machinelearningmastery.com/predicting-car-insurance-payout/]]
- https://medium.com/python-in-plain-english/building-a-smart-central-heating-system-with-a-raspberry-pi-and-python-403c6ea0fd7e

* On line Data

- [[https://www.kaggle.com/datasets][Kaggle]]

* Contact info

- Prof. dr. A.P. van Vuuren, a.p.van.vuuren@rug.nl
- dr. N.D. van Foreest, n.d.van.foreest@rug.nl)
