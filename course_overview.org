#+title: Bayesian Statistics, course outline
#+author: dr. N.D.  van Foreest
#+date: 2020:09:01

#+include: preamble.org
#+STARTUP: showall

* Schedule
We put the schedule on top for your convenience.

| Week | Schedule | Section                   | Lecturer |
|------+----------+---------------------------+----------|
|    1 |        1 | General                   | avv      |
|      |        2 | 2.1-2.5                   | avv      |
|    2 |        3 | 4.4                       | avv      |
|      |        4 | 5.1-5.3, 5.5-5.9, 6.1-6.2 | avv      |
|    3 |        5 | 7.1, 7.2, 7.4-7.6, 7.8    | avv      |
|      |        6 | Ch 8                      | avv      |
|    4 |        7 | Coding                    | nvf      |
|      |        8 |                           | nvf      |
|    5 |        9 |                           | nvf      |
|      |       10 |                           | nvf      |



* Course goals

After this course the student should be able to understand

1. the relationship between machine learning and classical econometrics.
2. the principles of machine learning methods including model selection based on cross validation.
3. understand the main algorithms of machine learning such as Ridge, Lasso, Nearest neigbors and trees.
4. Implement the core of the algorithms  in Python (R, C++, etc) code. The aim is to understand how the algorithms work.
5. Apply the algorithms on realistic practical problems with or without the help of statistical data packages such as scikit-learn.

* Course description

The last couple of decades of statistical data analysis have been characterized by an increased emphasis on algorithms that can be described under the name of machine learning or big data. These methods have been successfully applied to the prediction of face recognition (Facebook), and predicting consumer preferences for movies (Netflix). They are also widely used in industry to  decide whether just produced items match some quality criteria or not.  Finallly, in medicine algorithms can help staff to classify tumors, and so on.

In this course we consider the principles of this algorithmic approach during  in the lecture. We also look at practical examples and apply the algorithms in Python and compare our outcomes with existing software such as scikit-learn.

The topics that we deal with are the following:
1. Introductory to machine learning for the econometrician.
2. Principles of machine learning and kernel estimation methods.
3. Nonparametric estimation methods
4. Linear algorithms for prediction (Lasso, ridge)
5. Algorithms for classification.
6. Trees, bagging, random forests and boosting.




* Exam and Grading


** Report

Students in groups of two write a report.
Each group has to choose a method (from the primary book, or from one of the other sourcesbelow) which the members of the group are interested in learning.
The group has to find a problem on the internet with data to which to apply their method of choice.
The group writes a report, in LaTeX, of at most 10 pages, with name "groupno-title.pdf".
The contents are like this:

- Cover page: title, student names and student id, course code, year
- Core:
  - Intro, problem description
  - Data description (origin of data, data analysis/outliers, filtering methods applied), discussion of why that data is suitable. Use graphs where possible. Include small tables to explain the format of the data.
  - Methods: Why the method is chosen, suitability for the problem at hand, but also potential shortcomings.
  - Analysis, findings; use and discuss graphs.
  - Conclusion: main findings, quality/reliability of findings, evaluation of chosen methods, and extensions.
- Appendix with code whose core parts are documented

The report is due at Friday 23.59 of week 7 of the course.

** Oral exam

At the end of the course we invite the groups to explain their work in a oral exam, and we determine a grade


** Review of other report
Each group has to review the report of an other group.

** Grade
 80 % report, 10 % oral exam, 10 % review.

* Literature

Our primary book is [[https://people.smp.uq.edu.au/DirkKroese/DSML/][Kroeze, Data Science and Machine Learning]].

There are other, non-obligatory, sources that you might like to consult:

- [[https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf][James, Witen, Hastie, Tibshirani: Introduction to Stastical Learning]]
- [[https://web.stanford.edu/~hastie/ElemStatLearn/][Hastie, Tibshirani, and Friedman:The Elements of Statistical Learning]]
- [[https://web.stanford.edu/~hastie/CASI/][Hastie and Efron, Computer Age Statistical Inference: Algorithms, Evidence and Data Science]]
- [[https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf][Bishop, Pattern Recognition and Machine Learning]]




*  Interesting research topics
Here are some topics that you might find interesting. They may serve as inspiration for your own paper.


- https://www.sciencealert.com/mammal-extinctions-are-speeding-up-with-unprecedented-magnitude-scientists-warn
- https://www.pnas.org/content/114/37/9859
- https://www.theseattledataguy.com/healthcare-fraud-detection-with-python/
- [[https://towardsdatascience.com/learn-python-data-analytics-by-example-ny-parking-violations-e1ce1847fa2]]
- [[https://machinelearningmastery.com/random-forest-for-time-series-forecasting/]]
- [[https://machinelearningmastery.com/predicting-car-insurance-payout/]]
- https://medium.com/python-in-plain-english/building-a-smart-central-heating-system-with-a-raspberry-pi-and-python-403c6ea0fd7e

* On line Data

- [[https://www.kaggle.com/datasets][Kaggle]]

* Contact info

- Prof. dr. A.P. van Vuuren, a.p.van.vuuren@rug.nl
- dr. N.D. van Foreest, n.d.van.foreest@rug.nl)
