#+title: Machine Learning: algorithms, Code Lecture 4
#+subtitle: Trees
#+author: Nicky van Foreest
#+date: \today


#+STARTUP: overview
#+OPTIONS:  toc:1 H:3

#+PROPERTY: header-args    :session

#+include: preamble_presentation.org
# +include: preamble_handout.org


* Configuration for lecture                                        :noexport:

# +begin_src emacs-lisp
(setq org-format-latex-options (plist-put org-format-latex-options :scale 4.0))
(modus-themes-load-operandi)
(set-face-attribute 'default nil :height 220)
#+end_src



And config back
#+begin_src emacs-lisp
(setq org-format-latex-options (plist-put org-format-latex-options :scale 2.0))
(modus-themes-load-vivendi)
(set-face-attribute 'default nil :height 95)
#+end_src

#+RESULTS:

C-u C-u C-c C-x C-l to preview all

* Overview
- Last lecture
  - LP solvers and graphs
  - ridge
  - lasso
- This lecture
  - Recursion, examples
  - Decision Trees (Trees are recursive)
- Next lecture:
  - Bagging
  - Random Forests
  - Trees with weights.

* Recursion
Recursion is an important idea in the design of computer algorithms. Here I discuss two, to help you get in the mood. Then I'll apply it to make decision trees.
** The dumbest way to sort (hence, make recursion shine)
 Arrays  can be solved very fast with recursion.  But to see how sorting works, let us first see how /not/ to solve an array.

#+begin_src python :results output :exports both
def bublle_sort_never_use_this_as_it_is_the_dummest_algorithm_to_sort(L):
    for i in range(len(L)):
        for j in range(i + 1, len(L)):
            if L[i] > L[j]:
                L[i], L[j] = L[j], L[i]  # reverse


L = [3, 4, 1, 8, 10]
bublle_sort_never_use_this_as_it_is_the_dummest_algorithm_to_sort(L)
print(L)
#+end_src


** Quicksort

Sorting becomes much faster when using recursion. In fact, we go from  an order $n (n-1)/2 \sim O(n^{2})$ algorithm (bubble sort) to an $O(n\log n)$ algorithm.

#+begin_src python :results none :exports code
def qsort(xs):
    if len(xs) <= 1:
        return xs
    left, right, pivot = [], [], xs[0]
    for x in xs[1:]:
        if x < pivot:
            left.append(x)
        if x >= pivot:
            right.append(x)
    return qsort(left) + [pivot] + qsort(right)
#+end_src

- Note the  boundary/stopping condition here: it's the check on the length of the list ~len(xs) <=1~. When using recursion, always think about the stopping condition. If you mis a corner case, your code will keep running. Often it will claim more and more memory, and your computer will crash.
- /All/ sorting algoriths that you use on  the contact list on your phone,  spotify playlists, databases, use recursions.
- Don't /use/ the above code, as we do not cope with many corner cases. The idea is that you understand how recursion works in combination with sorting.

** Stationary distribution for the M/M/1 queue                    :noexport:

At a queueing system, jobs arrive at a single-server station as a Poisson process with rate $\lambda$; service times are exponentially distributed with mean $1/\mu$. Then, the stationary distribution satisfies the recursion
\begin{equation}
\label{eq:2}
\pi(i) = \pi(i-1) \lambda / \mu.
\end{equation}
Suppose at most $K$ jobs fit into the system. Then we can use this code to computate $\pi$.

#+begin_src python :results none :exports code
labda, mu= 3, 4

def pi(i):
    if i == 0:
        return 1
    return pi(i-1)*labda/mu


#+end_src

** Betting, memoizatoin and boundary conditions.
We have Peter and Paul betting on the outcome of a coin for $t$ rounds.
Peter wins a dollar if it lands heads, which happens with probability $p$; if it lands tails, Paul wins a dollar.
Peter starts with $i$ dollars, Paul with $n-i$ dollars.
Peter wins when during the course of the game he ends up with all $n$ dollars.
When he has not won all $n$ dollars before $t=0$, or Paul won all $n$ dollars, Peter looses.

*** What is the probability that Peter wins?

Let $u_{t, i}$ denote the probability that Peter wins if he has $i$ dollars. Clearly, $u_{t, i}$ satisfies the recursion:
\begin{align}
u_{t, i} &= p u_{t-1, i+1}  + q u_{t-1, i-1}, \\
u_{t, n} &= 1, \quad t\geq 0, \\
u_{t, 0} &= 0, \quad t\geq 0, \\
u_{0, i} &= 0, \quad 0\leq i < n.
\end{align}
Note again that we have to deal with /boundary conditions/, just as with quicksort.

Let's turn this into code. I use /memoization/ (check the web), as this leads to nice and readable code. Note that in the code, we first have to check the boundary condtions. Only when we /fell through/ all conditions, we can apply the recursion.
#+begin_src python :results output :exports both
from functools import lru_cache

p = 26/50
q = 1 - p
n = 10


@lru_cache()
def u(t, i):
    if i == n:
        return 1
    if i == 0:
        return 0
    if t == 0:
        return 0
    return p * u(t - 1, i + 1) + q * u(t - 1, i - 1)


print(u(100, 5))
#+end_src

#+RESULTS:
: 0.5939863359639174
A small detour: for the mathematically interested. The following problem (in which we removed the dependence on $t$) is a two-point boundary value problem:
\begin{align}
u(i) &= p u( i+1)  + q u_{i-1},\\
u(n) &= 1, \\
u(0) &= 0.
\end{align}
It is simple to show that this problem has an analytic solution, but in general it is much harder to solve (numerically) two-boundary problems then differential equations with only initial conditions.

*** What is the expected duration of the game?

Clearly, the game ends when either of the boundaries $0$ or $n$ is hit, or when Peter and Paul run out of rounds.
The recursions are simple. Let $v(t, i)$ be the expected duration of the game when there are at most $t$ rounds left and Peter starts with $i$ dollars. Then,
\begin{align}
v_{t, i} &= 1 + p v_{t-1, i+1}  + q v_{t-1, i-1}, \\
v_{t, n} &= v(t, 0) = 0, \quad t\geq 0, \\
v_{0, i} &= 0, \quad \text{for all } i.
\end{align}
#+begin_src python :results output :exports both
from functools import lru_cache

p = 26/50
q = 1 - p
n = 10


@lru_cache()
def v(t, i):
    if i == n or i == 0 or t==0:
        return 0
    return 1 + p * v(t - 1, i + 1) + q * v(t - 1, i - 1)


print(v(100, 5))
#+end_src




* Decision Trees

To build a decision tree, I started reading the code of DSML, but I did not really like the implementation.
I found it hard to understand, and (worse perhaps) I found  the code somewhat of ugly, and unpyhonic.

I tried to find another resource on the internet.
Searching on the web on ~scratch decision tree~ led me to [[https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/][this useful site]].
However, this also was not the way I like to see trees implemented.
In particular, they do not use classes, which makes the code a bit unorganized. In fact,
this is a missed opportunity because for trees have a recursive structure, which can be very nicely built with classes.
Moveover, I want to use numpy to speed up things.
(There also points that confuse me, like the use of ~row~ and ~index~.
I find it hard to memorize that ~index~ refers to a column of ~X~, that a ~row~ is a row of ~X~ (why not call it ~X[i,:]~?), and that ~row[-1]~ is the label ~Y~.)

So, I decided to build my own tree, but steal what seemed the best of the code of DSML and the above site.

You can find my code on ~github~ in ~tree_simple.py~.  With the code below I just want to illustrate how such things are built. For real work, use the classes of ~sklearn~, for instnce, the class ~DecisionTreeClassifier~ of ~sklearn~ is considerably more sophisticated (but also harder to understand).

** Score functions
I already know that I'll need  ascore function, so that is what I am going to build first.

Before building the score, we need  $p(z)$:
\begin{align}
p_{z} &= \frac{1}{|\sigma|} \sum_{y \in \sigma}\1{y=z}.
\end{align}

# Later, when we will consider /boosting/ we need a more general function $p(z)$ that takes into account the weights of (mis)classifying a point. When $w(y)$ is this weight, with the vector $w$ such that $\sum_{y\in \sigma} w(y) = 1$, then define
# \begin{align}
# p_{z} &= \sum_{y \in \sigma} w(y)\1{y=z}.
# \end{align}
# In case $w(y) = 1/|\sigma|$, we retrieve our earlier function


I'll use Gini's impurity function to score the misclassificaton of the tree. From the definitions of DSML:
\begin{align}
\label{eq:1}
G &= \frac{1}{2}\left(1-\sum_z p_z^2\right).
\end{align}

With $p(z)$ it's easy to also build the misclassification and entropy scores.

#+name: score_functions
#+begin_src python :results none :exports code
def p(Y, z):
    return (Y == z).sum() / len(Y)


def missclassify( Y):
    res = np.array([p(Y, z) for z in np.unique(Y)])
    return 1 - res.max(initial=0)


def entropy(Y):
    res = np.array([p(Y, z) for z in np.unique(Y)])
    return -res @ np.log(res)


def gini(Y):
    res = np.array([p(Y, z) for z in np.unique(Y)])
    return (1 - res @ res) / 2
#+end_src

 Below you will see that I'll associate a score function to the ~Tree~ class.

** A tree class

A tree is a bunch of branches, but branches can have sub branches, and so on. But, since a branch behaves like a tree, it suffices to just have one ~Tree~ class. Left and right branches are then also trees.

You should realize now that we are only considering /binary/ classification. So, we split in a /left/ and a /right/ subtree; there will not be a /middle/ tree, or anything else.

I use two class attributes: ~Tree.max_depth~ and ~Tree.min_size~.  We don't want to split a tree when it has less than  ~min_size~ datapoints.
We also don't want to split when a tree's depths exceeds ~max_depth~. As all subtrees in one tree must satisfy these constraints, it's is convenient to set the values as class attributes.


#+name: class_tree
#+begin_src python :results none :exports code
class Tree:
    def __init__(self, depth=0, min_size=1, max_depth=1):
        self.X, self.Y = None, None
        self.depth = depth
        self.left, self.right = None, None
        self.split_col = None
        self.split_value = None
        Tree.min_size = min_size
        Tree.max_depth = max_depth
#+end_src

The ~X~ is the feature vector, ~Y~ the response, ~left~ and ~right~ refer to a left and right tree, if present.
Finally, I use ~split_row~ and ~split_col~ to save the row and column of ~X~ that are optimal to split, the ~split_value=X[split_row, split_col]~ is the value to use in the optimal split.

** Two convenience methods

The size of a tree is determined by the number of datapoints it contains.
For this I can just check on ~len(self.Y)~, but I prefer to read ~self.size()~; this saves me thinking what about the meaning of ~len(self.Y)~. (Later, perhaps even tomorrow, I might be thinking why I am interested in the length of ~Y~, while ~size~ tells me directly what I wanted to do.)

It's more robust to call the ~score~ method everywhere. This hides the details of the exact score function I want to use. If later I would prefer  another score function, I can leave the rest code untouched; I only have the change  ~gini~ to, e.g., ~entropy~. The argument ~s~ is a vector with true/false values. When ~s[i] = True~, we include ~Y[i]~ in the score, otherwise not.

#+NAME: convenience
#+BEGIN_SRC python :noweb yes :results none :exports code
def size(self):
    return len(self.Y)

def score(self, s):
    return gini(self.Y[s])
#+end_src

Later, when building trees with weights, we'll see that it's convenient to pass a selector ~s~ to the score method, rather than ~self.Y[s]~.

** Score of a split

We want to split the data ~X~ along column ~j~.
Then for a given ~value~, we should put all rows ~X[row,col] <= value~ in the left tree, and the rest in the right tree.
The selector ~s=X[:,j] <= value~ gives all rows that have to go the left tree.
The value we need to split is the $i$th element of $X$ in column $j$.
By inverting the selector, i.e., ~~s~, I get data that thas to go the right tree.

The value to use in the split is given by ~X[i,j]~.

The score of the split is the score of the left and right nodes, each weigthed according to their size.


#+name: score_of_a_split
#+begin_src python :results none :exports code
def score_of_split(self, i, j):
    s = self.X[:, j] <= self.X[i, j]
    l_score = self.score(s) * len(self.Y[s]) / len(self.Y)
    r_score = self.score(~s) * len(self.Y[~s]) / len(self.Y)
    return l_score + r_score
#+end_src

** Optimal split

I have to find the best split of the data. For this, I run over each column in ~self.X~. Then for a given column, I find the best value to split the rows. This best value must be an element of ~self.X~, so I keep a reference to the best column and best row.

Note that we can take ~self.score(Y)~ as upper bound on the split. If none of the splits results in a lower score, we shouldn't split.


#+name: optimal_split
#+begin_src python :results none :exports code
def find_optimal_split(self):
    n, p = self.X.shape
    best_score = self.score([True] * len(self.Y))
    best_row = None
    best_col = None
    for j in range(p):
        for i in range(n):
            score = self.score_of_split(i, j)
            if score < best_score:
                best_score, best_row, best_col = score, i, j
    self.split_row = best_row
    self.split_col = best_col
    self.split_value = self.X[best_row, best_col]
#+end_src

** Do the split

Once I know the optimal split, I just have to implement the split.
However, if the size of the tree is too small of if ~max_depth~ is reached, I should not split.
Also, if the best split does not reduce the score, then ~self.split_row~ is still ~None~, and we should not split.
Finally, if after an optimal split, the left or right tree is empty, there is also nothing to split.

If we can make a good split, then do it, and then fit the left and right tree to the subsets of the data each of them receaves.
Since these trees are instances of ~Tree~, we can leave the process of fitting entirely to each of the trees themselves.
This is the beauty, but perhaps also a bit of the magic, of recursion.


#+name: do_split
#+begin_src python :results none :exports code
def split(self):
    if self.size() <= self.min_size or self.depth >= self.max_depth:
        return
    self.find_optimal_split()
    if self.split_row == None:
        return
    s = self.X[:, self.split_col] <= self.split_value
    if s.all() or (~s).all():
        return
    self.left = Tree(depth=self.depth + 1)
    self.left.fit(self.X[s], self.Y[s])
    self.right = Tree(depth=self.depth + 1)
    self.right.fit(self.X[~s], self.Y[~s])

def fit(self, X, Y):
    self.X, self.Y = X, Y
    self.split()
#+end_src

** Terminate the split

I need to know when a tree is a terminal(end) tree, because when the search has reached a terminal node, the tree should return a classifier of the terminal node.
So, when is a tree a terminal tree?
Well, if a tree doesn't have a left and/or right tree, this tree is not split, hence must a be terminal node.

#+name: terminal
#+begin_src python :results none :exports code
def terminal(self):
    return self.left == None or self.right == None
#+end_src

** Majority vote

Here I use a majority vote to classify. ~np.unique~ gives all unique values in ~self.Y~ and also how often they occur. So, ~np.argmax(count)~ gives the index of the value that occurs most. If I return the value with that index, I get the values that occurs most often.


#+name: majority vote
#+begin_src python :results none :exports code
def majority_vote(self):
    values, counts = np.unique(self.Y, return_counts=True)
    return values[np.argmax(counts)]

#+end_src

** Predict

If the search to classify a new feature vector ~x~ (note that this is normal letter, not a capital) along the trees reaches a terminal tree, then return its majority vote.
Otherwise, for that tree I know I have to split along the optimal column ~self.split_col~ and compare ~x[self.split_col]~ with the best value ~self.split_value~.
Depending on the outcome, I can leave the rest of the prediction to the left or right tree.

The method ~predict~ is a convenience function that can be applied to a matrix $X$, say, of data, not just one observation.

#+name: predict
#+begin_src python :results none :exports code
def _predict(self, x):
    if self.terminal():
        return self.majority_vote()
    if x[self.split_col] <= self.split_value:
        return self.left._predict(x)
    else:
        return self.right._predict(x)

def predict(self, X):
    return np.array([self._predict(x) for x in X])
#+end_src
** A test

Here is some test code.

#+name: tests
#+begin_src python :results none :exports code
def test():
    X = np.array(
        [
            [2.771244718, 1.784783929],
            [1.728571309, 1.169761413],
            [3.678319846, 2.81281357],
            [3.961043357, 2.61995032],
            [2.999208922, 2.209014212],
            [7.497545867, 3.162953546],
            [9.00220326, 3.339047188],
            [7.444542326, 0.476683375],
            [10.12493903, 3.234550982],
            [6.642287351, 3.319983761],
        ]
    )
    Y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])
    Y = 2 * Y - 1


    Tree.max_depth = 1
    Tree.min_size = 1
    tree = Tree()
    tree.fit(X, Y)
    print(tree.predict(X))
#+end_src

My final advice: Think hard about how classes work and recursion.
We'll use the ~Tree~ class again when we will deal with boosting.
The tree class we will use then is more difficult because we have to deal with weighted score functions.


** Tangle                                                         :noexport:


#+begin_src python :noweb yes :exports code :tangle ../code/tree_simple.py
import numpy as np

<<score_functions>>


<<class_tree>>

    <<convenience>>

    <<score_of_a_split>>

    <<optimal_split>>

    <<do_split>>

    <<terminal>>

    <<majority vote>>

    <<predict>>


<<tests>>


if __name__ == "__main__":
    test()
#+end_src

#+RESULTS:


* Exercises

1. Why is Quicksort an $O(n\log n)$ algorithm? (This helps you a lot in understanding how recursion works!)
2. Define  (and do) some tests for the ~Tree~ class.  Here are some simple (and dumb) things to test:
   a. Test on ~min_size=0~? Should the tree crash? What should happen in that case?
   b. Test on  ~max_depth=np.infty~? Then the size of each branch should be at most ~min_size~ large. Is that indeed true?
   c. What happens if you pass ~X=[]~, i.e., an empty array to the tree?
   d. What if all values of ~X~ are the same?
   e. What if all values of ~Y~ are the same?
   f. What if just one value of ~Y~ is different?
   g. Can we mix positive and negative values in ~Y~ and/or ~X~? Of course, all should work, but does it actually?
   h. If you like, add some error checking. For instance, ensure that the shapes of $X$, $Y$ and $w$ are compatible. Are the divisions by zero that can give strange errors?
3. Compare the code of DSML and  [[https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/][this site]] to my code.
   a. What parts of each code do you like? Why?
   b. Can you see what ideas of the others I copied?
   c. Do you understand why I changed certain parts?
   d. Think about how you would build your code based on these three examples of code.
