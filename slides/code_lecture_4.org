#+title: Machine Learning: algorithms, Code Lecture 4
#+subtitle: Trees
#+author: Nicky van Foreest
#+date: \today


#+STARTUP: overview
#+OPTIONS:  toc:1 H:2

#+PROPERTY: header-args    :session

# +include: preamble_presentation.org
#+include: preamble_handout.org


* Configuration for lecture                                        :noexport:

# +begin_src emacs-lisp
(setq org-format-latex-options (plist-put org-format-latex-options :scale 4.0))
(modus-themes-load-operandi)
(set-face-attribute 'default nil :height 220)
#+end_src

And config back
#+begin_src emacs-lisp
(setq org-format-latex-options (plist-put org-format-latex-options :scale 2.0))
(modus-themes-load-vivendi)
(set-face-attribute 'default nil :height 95)
#+end_src

#+RESULTS:

C-u C-u C-c C-x C-l to preview all

* Overview
- Last lecture
  - LP solvers and graphs
  - ridge
  - lasso
- This lecture
  - Decision Trees
- Next lecture:

Recall, the goal is to provide you with working code and explain how the code works.

* Recursion
Recursion is an important idea in the design of computer algorithms. Here I discuss two, to help you get in the mood. Then I'll apply it to make decision trees.
** Quicksort
The goal is to explain how recursion is used in sorting. Don't implement this, as we do not cope with many corner cases.

#+begin_src python :results none :exports code
def qsort(xs):
    if len(xs) <= 1:
        return xs
    left, right, pivot = [], [], xs[0]
    for x in xs[1:]:
        if x < pivot:
            left.append(x)
        if x >= pivot:
            right.append(x)
    return qsort(left) + [pivot] + qsort(right)
#+end_src

Note the  boundary/stopping condition here: it's the check on the length of the list ~len(xs) <=1~. When using recursion, always think about the stopping condition. If you mis a corner case, your code will keep running. Often it will claim more and more memory, and your computer will crash.
** Stationary distribution for the M/M/1 queue                    :noexport:

At a queueing system, jobs arrive at a single-server station as a Poisson process with rate $\lambda$; service times are exponentially distributed with mean $1/\mu$. Then, the stationary distribution satisfies the recursion
\begin{equation}
\label{eq:2}
\pi(i) = \pi(i-1) \lambda / \mu.
\end{equation}
Suppose at most $K$ jobs fit into the system. Then we can use this code to computate $\pi$.

#+begin_src python :results none :exports code
labda, mu= 3, 4

def pi(i):
    if i == 0:
        return 1
    return pi(i-1)*labda/mu


#+end_src

** Peter and Paul, memoizatoin and boundary conditions.
We have Peter and Paul betting on the outcome of a coin a number of $t$ rounds.
Peter wins a dollar if it lands heads, which happens with probability $p$; if it lands tails, Paul wins a dollar. Peter and Paul both start with 5 dollar. Peter wins when during the course of the game he ends up with all 10 dollars. When he has not won all 10 dollars before $t=0$, or Paul has all 10 dollars, Peter looses. What is the probability that Peter wins?

Let $u_{t, i}$ denote the probability that Peter wins if he has $i$ dollars. Clearly, $u_{t, i}$ satisfies the recursion:
\begin{align}
u(t, i) &= p u(t-1, i+1)  + q u_{t-1, i-1},
\end{align}
and $u(t, n) = 1$ for all $t$ and $n=10$, and $u_{0, i}=0$ for all $i\leq n=10$.

Let's turn this into code. I use /memoization/ (check the web), as this leads to extremely nice and readable code.
#+begin_src python :results output :exports results
from functools import lru_cache

p = 26/50
q = 1 - p
n = 10


@lru_cache()
def u(t, i):
    if i == n:
        return 1
    if i == 0:
        return 0
    if t == 0:
        return 0
    return p * u(t - 1, i + 1) + q * u(t - 1, i - 1)


print(u(100, 5))
#+end_src

#+RESULTS:
: 0.5939863359639174

Here is small detour. Interstingly, the following problem (in which we removed the dependence on $t$) is a two-point boundary value problem:
\begin{align}
u(i) &= p u( i+1)  + q u_{i-1},\\
u(n) &= 1, \\
u(0) &= 0.
\end{align}
It is simple to show that this problem has an analytic solution, but to find it numerically is considerably harder.

* Decision Trees

To build a decision tree, I started reading the code of DSML, but I did not really like the implementation.
I found it hard to understand, and (worse perhaps) I found  the code somewhat of ugly, and unpyhonic.

I  tried to find another resource on the internet.
Searching on the web on ~scratch, decision, tree~ led me to [[https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/][this useful site]].
However, this also was not the way I like to see trees implemented.
For instance, the code does not use classes, which makes the code a bit unorganized.
There also points that confuse me, like the use of ~row~ and ~index~.
I find it hard to memorize that ~index~ refers to a column of ~X~, that a ~row~ is a row of ~X~ (why not call it ~X[i,:]~?), and that ~row[-1]~ is the label ~Y~. Then, they don't use classes,  but for trees  classes shine , due to the recursive structure. Moveover, I want to use numpy to speed up things.

So, I decided to build my own tree, but steal what seemed the best of the code of the above two resources.

You can find the code on ~github~ in ~tree_simple.py~. The class ~DecisionTreeClassifier~ of ~sklearn~ is considerably more sophisticated. With the code below I just want to illustrate how such things are built. For real work, use the classes of ~sklearn~.

** Score functions
I know that I'll need a score function for a tree in the tree.

Before building the score, we need  $p(z)$:
\begin{align}
p_{z} &= \frac{1}{|\sigma|} \sum_{y \in \sigma}\1{y=z}.
\end{align}

# Later, when we will consider /boosting/ we need a more general function $p(z)$ that takes into account the weights of (mis)classifying a point. When $w(y)$ is this weight, with the vector $w$ such that $\sum_{y\in \sigma} w(y) = 1$, then define
# \begin{align}
# p_{z} &= \sum_{y \in \sigma} w(y)\1{y=z}.
# \end{align}
# In case $w(y) = 1/|\sigma|$, we retrieve our earlier function


I'll use Gini's impurity function to score the misclassificaton of the tree. From the definitions of DSML:
\begin{align}
\label{eq:1}
G &= \frac{1}{2}\left(1-\sum_z p_z^2\right).
\end{align}

With $p(z)$ it's easy to also build the misclassification and entropy scores.

#+name: score_functions
#+begin_src python :results none :exports code
def p(Y, z):
    return (Y == z).sum() / len(Y)


def missclassify( Y):
    res = np.array([p(Y, z) for z in np.unique(Y)])
    return 1 - res.max(initial=0)


def entropy(Y):
    res = np.array([p(Y, z) for z in np.unique(Y)])
    return -res @ np.log(res)


def gini(Y):
    res = np.array([p(Y, z) for z in np.unique(Y)])
    return (1 - res @ res)/2
#+end_src

# Below you will see that I'll associate a score function to the ~Tree~ class. To call the score function as a class method, requires the first argument of the above functions be a class.

** A tree class

A tree is a bunch of branches, but branches can have sub branches, and so on. But, since a branch behaves like a tree, it suffices to just have one ~Tree~ class. Left and right branches are also trees.

I use two class attributes.  We don't want to split a tree when it has less than  ~min_size~ datapoints.
We also don't want to split when a tree's depths exceeds ~max_depth~. As all subtrees in one tree must satisfy these constraints, it's is convenient to set the values as class attributes.


#+name: class_tree
#+begin_src python :results none :exports code
class Tree:
    min_size = 1
    max_depth = 0
#+end_src

** Initialize an instance

The ~X~ is the feature vector, ~Y~ the response, ~left~ and ~right~ refer to a left and right tree, if present. Finally, I use ~split_row~ and ~split_col~ to save the row and  column of ~X~ that are optimal to split, the ~split_value=X[split_row, split_col]~ is the value to use in the optimal split.

#+name: init
#+begin_src python :results none :exports code
def __init__(self, depth=0):
    self.X, self.Y = None, None
    self.depth = depth
    self.left, self.right = None, None
    self.split_row = None
    self.split_col = None
    self.split_value = None
#+end_src

** Two convenience methods

The size of a tree is determined by the number of datapoints it contains.
For this I can just check on ~len(self.Y)~, but I prefer to read ~self.size()~; this saves me thinking what about the meaning of ~len(self.Y)~. (Later, perhaps even tomorrow, I might be thinking why I am interested in the length of ~Y~, while ~size~ tells me directly what I wanted to do.)

It's more robust to call the ~score~ method everywhere. This hides the details of the exact score function I want to use. If later I would prefer  another score function, I can leave the rest code untouched; I only have the change  ~gini~ to, e.g., ~entropy~.

#+NAME: convenience
#+BEGIN_SRC python :noweb yes :exports code
def size(self):
    return len(self.Y)

def score(self, Y):
    return gini(Y)
#+end_src

#+RESULTS: convenience

** Score of a split

We want to split the data ~X~ along column ~j~.
Then for a given ~value~, we should put all rows ~X[row,col] <= value~ in the left tree, and the rest in the right tree.
The selector ~s=X[:,j] < value~ gives all rows that have to go the left tree.
The value we need to split is the $i$th element of $X$ in column $j$.
By inverting the selector, i.e., ~~s~, I get data that thas to go the right tree.

The value to use in the split is given by ~X[i,j]~.

The score of the split is the score of the left and right nodes, each weigthed according to their size.


#+name: score_of_a_split
#+begin_src python :results none :exports code
def score_of_split(self, i, j):
    s = self.X[:, j] <= self.X[i, j]
    l_score = self.score(self.Y[s])
    r_score = self.score(self.Y[~s])
    tot = l_score * len(self.Y[s]) / len(self.Y)
    tot += r_score * len(self.Y[~s]) / len(self.Y)
    return tot
#+end_src

** Optimal split

I have to find the best split of the data. For this, I run over each column in ~self.X~. Then for a given column, I find the best value to split the rows. This best value must be an element of ~self.X~, so I keep a reference to the best column and best row.

Note that we can take ~self.score(Y)~ as upper bound on the split. If none of the splits results in a lower score, we shouldn't split.


#+name: optimal_split
#+begin_src python :results none :exports code
def find_optimal_split(self):
    n, p = self.X.shape
    best_score = self.score(self.Y)
    best_row = None
    best_col = None
    for j in range(p):
        for i in range(n):
            score = self.score_of_split(i, j)
            if score < best_score:
                best_score, best_row, best_col = score, i, j
                # print("best", i, j, score)
    self.split_row = best_row
    self.split_col = best_col
    self.split_value = self.X[best_row, best_col]
#+end_src

** Do the split

Once I know the optimal split, I just have to implement the split.
However, if the size of the tree is too small of if the ~max_depth~ is reached, I should not split.
Also, the best split does not reduce the score, then ~self.split_row~ is still ~None~, and we should not split.
Finally, after an optimal split, the left or right tree is empty, there is nothting to split.

If there is a split, to make, then do it, and see recursively if the left and right tree need to be split also.
Since these trees are instances of ~Tree~, we can leave the process of splitting entirely to each of the trees themselves.
This is the beauty, but perhaps also a bit of the magic, of recursion.


#+name: do_split
#+begin_src python :results none :exports code
def split(self):
    if self.size() <= self.min_size or self.depth >= self.max_depth:
        return
    self.find_optimal_split()
    if self.split_col == None:
        return
    s = self.X[:, self.split_col] < self.split_value
    if s.all() or (~s).all():
        return
    self.left = Tree(depth=self.depth + 1)
    self.left.fit(self.X[s], self.Y[s])
    self.right = Tree(depth=self.depth + 1)
    self.right.fit(self.X[~s], self.Y[~s])

def fit(self, X, Y):
    self.X, self.Y = X, Y
    self.split()
#+end_src

** Terminate the split

I need to know when a tree is a terminal(end) tree, because when the search has reached a terminal node, the tree should return a classifier of the terminal node.
So, when is a tree a terminal tree?
Well, if a tree doesn't have a left and/or right tree, this tree is not split, hence must a be terminal node.

#+name: terminal
#+begin_src python :results none :exports code
def terminal(self):
    return self.left == None or self.right == None
#+end_src

** Majority vote

Here I use a majority vote to classify. ~np.unique~ gives all unique values in ~self.Y~ and also how often they occur. So, ~np.argmax(count)~ gives the index of the value that occurs most. If I return the value with that index, I get the values that occurs most often.


#+name: majority vote
#+begin_src python :results none :exports code
def majority_vote(self):
    values, counts = np.unique(self.Y, return_counts=True)
    return values[np.argmax(counts)]

#+end_src

** Predict

If the search to classify a new feature vector ~x~ (note that this is normal letter, not a capital) along the trees reaches a terminal tree, then return its majority vote.
Otherwise, for that tree I know I have to split along the optimal column ~self.split_col~ and compare ~x[self.split_col]~ with the best value ~self.split_value~.
Depending on the outcome, I can leave the rest of the prediction to the left or right tree.

#+name: predict
#+begin_src python :results none :exports code
def predict(self, x):
    if self.terminal():
        return self.majority_vote()
    if x[self.split_col] <= self.split_value:
        return self.left.predict(x)
    else:
        return self.right.predict(x)
#+end_src
** A test

Here is some test code.

#+name: tests
#+begin_src python :results none :exports code
def test():
    X = np.array(
        [
            [2.771244718, 1.784783929],
            [1.728571309, 1.169761413],
            [3.678319846, 2.81281357],
            [3.961043357, 2.61995032],
            [2.999208922, 2.209014212],
            [7.497545867, 3.162953546],
            [9.00220326, 3.339047188],
            [7.444542326, 0.476683375],
            [10.12493903, 3.234550982],
            [6.642287351, 3.319983761],
        ]
    )
    Y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])


    Tree.max_depth = 1
    Tree.min_size = 1
    tree = Tree()
    tree.fit(X, Y)

    for x in X:
        print(tree.predict(x))
#+end_src



** Tangle :noexport:

#+begin_src python :noweb yes :exports none :tangle ../code/bag.py
<<imports_bags>>

<<class_bag>>

    <<make_bag_trees>>

    <<predict>>

    <<print>>

<<test_bag>>

if __name__ == "__main__":
    test()
#+end_src

#+RESULTS:





#+begin_src python :noweb yes :exports code :tangle ../code/tree_simple.py
import numpy as np

<<score_functions>>


<<class_tree>>

    <<init>>

    <<convenience>>

    <<score_of_a_split>>

    <<optimal_split>>

    <<do_split>>

    <<terminal>>

    <<majority vote>>

    <<predict>>


<<tests>>


if __name__ == "__main__":
    test()
#+end_src

#+RESULTS:


* Exercises

1. Define  (and do) some tests for the ~Tree~ class.  Here are some simple (and dumb) things to test:
   a. Test on ~min_size=0~? Should the tree crash? What should happen in that case?
   b. Test on  ~max_depth=np.infty~? Then the size of each branch should be at most ~min_size~ large. Is that indeed true?
   c. What happens if you pass ~X=[]~, i.e., an empty array to the tree?
   d. What if all values of ~X~ are the same?
   e. What if all values of ~Y~ are the same?
   f. What if just one value of ~Y~ is different?
   g. Can we mix positive and negative values in ~Y~ and/or ~X~? Of course, all should work, but does it actually?
   h. If you like, add some error checking. For instance, ensure that the shapes of $X$, $Y$ and $w$ are compatible. Are the divisions by zero that can give strange errors?
2. Compare the code of DSML and  [[https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/][this site]] to my code.
   a. What parts of each code do you like? Why?
   b. Can you see what ideas of the others I copied?
   c. Do you understand why I changed certain parts?
   d. Think about how you would build your code based on these three examples of code.
